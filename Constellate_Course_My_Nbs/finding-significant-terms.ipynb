{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n\nCreated by [Nathan Kelber](http://nkelber.com) and Ted Lawless for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\nFor questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n___\n\n# Finding Significant Words Using TF/IDF\n\n**Description:**\nThis [notebook](https://docs.constellate.org/key-terms/#jupyter-notebook) shows how to discover significant words. The method for finding significant terms is [tf-idf](https://docs.constellate.org/key-terms/#tf-idf).  The following processes are described:\n\n* An educational overview of TF-IDF, including how it is calculated\n* Using the `tdm_client` to retrieve a dataset\n* Filtering based on a pre-processed ID list\n* Filtering based on a [stop words list](https://docs.constellate.org/key-terms/#stop-words)\n* Cleaning the tokens in the dataset\n* Creating a [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary)\n* Creating a [gensim](https://docs.constellate.org/key-terms/#gensim) [bag of words](https://docs.constellate.org/key-terms/#bag-of-words) [corpus](https://docs.constellate.org/key-terms/#corpus)\n* Computing the most significant words in your [corpus](https://docs.constellate.org/key-terms/#corpus) using [gensim](https://docs.constellate.org/key-terms/#gensim) implementation of [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf)\n\n**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n\n[Take me to the **Research Version** of this notebook ->](./finding-significant-terms-for-research.ipynb)\n\n**Difficulty:** Intermediate\n\n**Completion time:** 60 minutes\n\n**Knowledge Required:** \n* Python Basics Series ([Start Python Basics I](./python-basics-1.ipynb))\n\n**Knowledge Recommended:**\n* [Exploring Metadata](./metadata.ipynb)\n* [Working with Dataset Files](./working-with-dataset-files.ipynb)\n* [Pandas I](./pandas-1.ipynb)\n* [Creating a Stopwords List](./creating-stopwords-list.ipynb)\n* A familiarity with [gensim](https://docs.constellate.org/key-terms/#gensim) is helpful but not required.\n\n**Data Format:** [JSON Lines (.jsonl)](https://docs.constellate.org/key-terms/#jsonl)\n\n**Libraries Used:**\n* `pandas` to load a preprocessing list\n* `csv` to load a custom stopwords list\n* [gensim](https://docs.constellate.org/key-terms/#gensim) to help compute the [tf-idf](https://docs.constellate.org/key-terms/#tf-idf) calculations\n* [NLTK](https://docs.constellate.org/key-terms/#nltk) to create a stopwords list (if no list is supplied)\n\n**Research Pipeline:**\n\n1. Build a dataset\n2. Create a \"Pre-Processing CSV\" with [Exploring Metadata](./exploring-metadata.ipynb) (Optional)\n3. Create a \"Custom Stopwords List\" with [Creating a Stopwords List](./creating-stopwords-list.ipynb) (Optional)\n4. Complete the TF-IDF analysis with this notebook\n____"},{"metadata":{},"cell_type":"markdown","source":"## What is \"Term Frequency- Inverse Document Frequency\" (TF-IDF)?\n\n[TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) is used in [machine learning](https://docs.constellate.org/key-terms/#machine-learning) and [natural language processing](https://docs.constellate.org/key-terms//#nlp) for measuring the significance of particular terms for a given document. It consists of two parts that are multiplied together:\n\n1. Term Frequency- A measure of how many times a given word appears in a document\n2. Inverse Document Frequency- A measure of how many times the same word occurs in other documents within the corpus\n\nIf we were to merely consider [word frequency](https://docs.constellate.org/key-terms/#word-frequency), the most frequent words would be common [function words](https://docs.constellate.org/key-terms/#function-words) like: \"the\", \"and\", \"of\". We could use a [stopwords list](https://docs.constellate.org/key-terms/#stop-words) to remove the common [function words](https://docs.constellate.org/key-terms/#function-words), but that still may not give us results that describe the unique terms in the document since the uniqueness of terms depends on the context of a larger body of documents. In other words, the same term could be significant or insignificant depending on the context. Consider these examples:\n\n* Given a set of scientific journal articles in biology, the term \"lab\" may not be significant since biologists often rely on and mention labs in their research. However, if the term \"lab\" were to occur frequently in a history or English article, then it is likely to be significant since humanities articles rarely discuss labs. \n* If we were to look at thousands of articles in literary studies, then the term \"postcolonial\" may be significant for any given article. However, if were to look at a few hundred articles on the topic of \"the global south,\" then the term \"postcolonial\" may occur so frequently that it is not a significant way to differentiate between the articles.\n\nThe [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) calculation reveals the words that are frequent in this document **yet rare in other documents**. The goal is to find out what is unique or remarkable about a document given the context (and *the given context* can change the results of the analysis).\n\nHere is how the calculation is mathematically written:\n\n$$tfidf_{t,d} = tf_{t,d} \\cdot idf_{t,D}$$\n\nIn plain English, this means: **The value of [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) is the product (or multiplication) of a given term's frequency multiplied by its inverse document frequency.** Let's unpack these terms one at a time.\n\n### Term Frequency Function\n\n$$tf_{t,d}$$\nThe number of times (t) a term occurs in a given document (d)\n\n### Inverse Document Frequency Function\n\n$$idf_i = \\mbox{log} \\frac{N}{|{d : t_i \\in d}|}$$\nThe inverse document frequency can be expanded to the calculation on the right. In plain English, this means: **The log of the total number of documents (N) divided by the number of documents that contain the term**\n\n### TF-IDF Calculation in Plain English\n\n$$(Times-the-word-occurs-in-given-document) \\cdot \\mbox{log} \\frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}$$\n\nThere are variations on the [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) formula, but this is the most widely-used version."},{"metadata":{},"cell_type":"markdown","source":"### An Example Calculation of TF-IDF\n\nLet's take a look at an example to illustrate the fundamentals of [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf). First, we need several texts to compare. Our texts will be very simple.\n\n* text1 = 'The grass was green and spread out the distance like the sea.'\n* text2 = 'Green eggs and ham were spread out like the book.'\n* text3 = 'Green sailors were met like the sea met troubles.'\n* text4 = 'The grass was green.'\n\nThe first step is we need to discover how many unique words are in each text. \n\n|text1|text2|text3|text4|\n|    ---    | ---| --- | --- |\n|the|green|green|the|\n|grass|eggs|sailors|grass|\n|was|and|were|was|\n|green|ham|met|green|\n|and|were|like| |\n|spread|spread|the| |\n|out|out|sea| |\n|into|like|met| |\n|distance|the|troubles| |\n|like|book| | |\n|sea| | | |\n\n\nOur four texts share some similar words. Next, we create a single list of unique words that occur across all three texts. (When we use the [gensim](https://docs.constellate.org/key-terms/#gensim) library later, we will call this list a [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary).)\n\n|id|Unique Words|\n|---| --- |\n|0|and|\n|1|book|\n|2|distance|\n|3|eggs|\n|4|grass|\n|5|green|\n|6|ham|\n|7|like|\n|8|met|\n|9|out|\n|10|sailors|\n|11|sea|\n|12|spread|\n|13|the|\n|14|troubles|\n|15|was|\n|16|were|\n\nNow let's count the occurences of each unique word in each sentence\n\n|id|word|text1|text2|text3|text4|\n|---|---|---|---|---|---|\n|0|and|1|1|0|0|\n|1|book|0|1|0|0|\n|2|distance|1|0|0|0|\n|3|eggs|0|1|0|0|\n|4|grass|1|0|0|1|\n|5|green|1|1|1|1|\n|6|ham|0|1|0|0|\n|7|like|1|1|1|0|\n|8|met|0|0|2|0|\n|9|out|1|1|0|0|\n|10|sailors|0|0|1|0|\n|11|sea|1|0|1|0|\n|12|spread|1|1|0|0|\n|13|the|3|1|1|1|\n|14|troubles|0|0|1|0|\n|15|was|1|0|0|1|\n|16|were|0|1|1|0|"},{"metadata":{},"cell_type":"markdown","source":"### Computing TF-IDF (Example 1)\n\nWe have enough information now to compute [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) for every word in our corpus. Recall the plain English formula.\n\n$$(Times-the-word-occurs-in-given-document) \\cdot \\mbox{log} \\frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}$$\n\nWe can use the formula to compute [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) for the most common word in our corpus: 'the'. In total, we will compute [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) four times (once for each of our texts).\n\n|id|word|text1|text2|text3|text4|\n|---|---|---|---|---|---|\n|13|the|3|1|1|1|\n\ntext1: $$ tf-idf = 3 \\cdot \\mbox{log} \\frac{4}{(4)} = 3 \\cdot \\mbox{log} 1 = 3 \\cdot 0 = 0$$\ntext2: $$ tf-idf = 1 \\cdot \\mbox{log} \\frac{4}{(4)} = 1 \\cdot \\mbox{log} 1 = 1 \\cdot 0 = 0$$\ntext3: $$ tf-idf = 1 \\cdot \\mbox{log} \\frac{4}{(4)} = 1 \\cdot \\mbox{log} 1 = 1 \\cdot 0 = 0$$\ntext4: $$ tf-idf = 1 \\cdot \\mbox{log} \\frac{4}{(4)} = 1 \\cdot \\mbox{log} 1 = 1 \\cdot 0 = 0$$\n\nThe results of our analysis suggest 'the' has a weight of 0 in every document. The word 'the' exists in all of our documents, and therefore it is not a significant term to differentiate one document from another.\n\nGiven that idf is\n\n$$\\mbox{log} \\frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}$$\n\nand \n\n$$\\mbox{log} 1 = 0$$\nwe can see that [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) will be 0 for any word that occurs in every document. That is, if a word occurs in every document, then it is not a significant term for any individual document."},{"metadata":{},"cell_type":"markdown","source":"### Computing TF-IDF (Example 2)\n\nLet's try a second example with the word 'out'. Recall the plain English formula.\n\n$$(Times-the-word-occurs-in-given-document) \\cdot \\mbox{log} \\frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}$$\n\nWe will compute [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) four times, once for each of our texts.\n\n|id|word|text1|text2|text3|text4|\n|---|---|---|---|---|---|\n|9|out|1|1|0|0|\n\ntext1: $$ tf-idf = 1 \\cdot \\mbox{log} \\frac{4}{(2)} = 1 \\cdot \\mbox{log} 2 = 1 \\cdot .3010 = .3010$$\ntext2: $$ tf-idf = 1 \\cdot \\mbox{log} \\frac{4}{(2)} = 1 \\cdot \\mbox{log} 2 = 1 \\cdot .3010 = .3010$$\ntext3: $$ tf-idf = 0 \\cdot \\mbox{log} \\frac{4}{(2)} = 0 \\cdot \\mbox{log} 2 = 0 \\cdot .3010 = 0$$\ntext4: $$ tf-idf = 0 \\cdot \\mbox{log} \\frac{4}{(2)} = 0 \\cdot \\mbox{log} 2 = 0 \\cdot .3010 = 0$$\n\nThe results of our analysis suggest 'out' has some significance in text1 and text2, but no significance for text3 and text4 where the word does not occur."},{"metadata":{},"cell_type":"markdown","source":"### Computing TF-IDF (Example 3)\n\nLet's try one last example with the word 'met'. Here's the [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) formula again:\n\n$$(Times-the-word-occurs-in-given-document) \\cdot \\mbox{log} \\frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}$$\n\nAnd here's how many times the word 'met' occurs in each text.\n\n|id|word|text1|text2|text3|text4|\n|---|---|---|---|---|---|\n|8|met|0|0|2|0|\n\ntext1: $$ tf-idf = 0 \\cdot \\log \\frac{4}{(1)} = 0 \\cdot \\mbox{log} 4 = 0 \\cdot .6021 = 0$$\ntext2: $$ tf-idf = 0 \\cdot \\mbox{log} \\frac{4}{(1)} = 0 \\cdot \\mbox{log} 4 = 0 \\cdot .6021 = 0$$\ntext3: $$ tf-idf = 2 \\cdot \\mbox{log} \\frac{4}{(1)} = 2 \\cdot \\mbox{log} 4 = 2 \\cdot .6021 = 1.2042$$\ntext4: $$ tf-idf = 0 \\cdot \\mbox{log} \\frac{4}{(1)} = 0 \\cdot \\mbox{log} 4 = 0 \\cdot .6021 = 0$$\n\nAs should be expected, we can see that the word 'met' is very significant in text3 but not significant in any other text since it does not occur in any other text. "},{"metadata":{},"cell_type":"markdown","source":"### The Full TF-IDF Example Table\n\nHere are the original sentences for each text:\n\n* text1 = 'The grass was green and spread out the distance like the sea.'\n* text2 = 'Green eggs and ham were spread out like the book.'\n* text3 = 'Green sailors were met like the sea met troubles.'\n* text4 = 'The grass was green.'\n\nAnd here's the corresponding [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) scores for each word in each text:\n\n|id|word|text1|text2|text3|text4|\n|---|---|---|---|---|---|\n|0|and|.3010|.3010|0|0|\n|1|book|0|.6021|0|0|\n|2|distance|.6021|0|0|0|\n|3|eggs|0|.6021|0|0|\n|4|grass|.3010|0|0|.3010|\n|5|green|0|0|0|0|\n|6|ham|0|.6021|0|0|\n|7|like|.1249|.1249|.1249|0|\n|8|met|0|0|1.2042|0|\n|9|out|.3010|.3010|0|0|\n|10|sailors|0|0|.6021|0|\n|11|sea|.3010|0|.3010|0|\n|12|spread|.3010|.3010|0|0|\n|13|the|0|0|0|0|\n|14|troubles|0|0|.6021|0|\n|15|was|.3010|0|0|.3010|\n|16|were|0|.3010|.3010|0|\n\nThere are a few noteworthy things in this data. \n\n* The [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) score for any word that does not occur in a text is 0.\n* The scores for almost every word in text4 are 0 since it is a shorter version of text1. There are no unique words in text4 since text1 contains all the same words. It is also a short text which means that there are only four words to consider. The words 'the' and 'green' occur in every text, leaving only 'was' and 'grass' which are also found in text1.\n* The words 'book', 'eggs', and 'ham' are significant in text2 since they only occur in that text.\n\nNow that you have a basic understanding of how [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) is computed at a small scale, let's try computing [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) on a [corpus](https://docs.constellate.org/key-terms/#corpus) which could contain millions of words.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"## Computing TF-IDF with your Dataset"},{"metadata":{},"cell_type":"markdown","source":"We'll use the tdm_client library to automatically retrieve the dataset in the JSON file format. \n\nEnter a [dataset ID](https://docs.constellate.org/key-terms/#dataset-ID) in the next code cell.\n\nIf you don't have a dataset ID, you can:\n* Use the sample dataset ID already in the code cell\n* [Create a new dataset](https://constellate.org/builder)\n* [Use a dataset ID from other pre-built sample datasets](https://constellate.org/dataset/dashboard)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset is \"geometric group theory,\" 1980-present\ndataset_id = \"a2b5c15a-8de3-e161-077b-8afd47713826\"","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, import the `tdm_client`, passing the `dataset_id` as an argument using the `get_dataset` method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing your dataset with a dataset ID\nimport tdm_client\n# Pull in the dataset that matches `dataset_id`\n# in the form of a gzipped JSON lines file.\ndataset_file = tdm_client.get_dataset(dataset_id)","execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Constellate: use and download of datasets is covered by the <a target=\"_blank\" href=\"https://tdm-pilot.org/terms-and-conditions/\">Terms & Conditions of Use</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p>\"geometric group theory\" from 1980 - 2021</p><p>1411 documents.</p>"},"metadata":{}},{"output_type":"stream","text":"INFO:root:Downloading a2b5c15a-8de3-e161-077b-8afd47713826 metadata to /home/jovyan/data/a2b5c15a-8de3-e161-077b-8afd47713826-sampled-jsonl.jsonl.gz\n","name":"stdout"},{"output_type":"stream","text":"100% |########################################################################|\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Apply Pre-Processing Filters (if available)\nIf you completed pre-processing with the \"Exploring Metadata and Pre-processing\" notebook, you can use your CSV file of dataset IDs to automatically filter the dataset. Your pre-processed CSV file  must be in the root folder."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import a pre-processed CSV file of filtered dataset IDs.\n# If you do not have a pre-processed CSV file, the analysis\n# will run on the full dataset and may take longer to complete.\nimport pandas as pd\nimport os\n\npre_processed_file_name = f'data/pre-processed_{dataset_id}.csv'\n\nif os.path.exists(pre_processed_file_name):\n    df = pd.read_csv(pre_processed_file_name)\n    filtered_id_list = df[\"id\"].tolist()\n    use_filtered_list = True\n    print('Pre-Processed CSV found. Successfully read in ' + str(len(df)) + ' documents.')\nelse: \n    use_filtered_list = False\n    print('No pre-processed CSV file found. Full dataset will be used.')","execution_count":3,"outputs":[{"output_type":"stream","text":"INFO:numexpr.utils:NumExpr defaulting to 4 threads.\nPre-Processed CSV found. Successfully read in 634 documents.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Load Stopwords List\n\nIf you have created a stopword list in the stopwords notebook, we will import it here. (You can always modify the CSV file to add or subtract words then reload the list.) Otherwise, we'll load the NLTK [stopwords](https://docs.constellate.org/key-terms/#stop-words) list automatically."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load a custom data/stop_words.csv if available\n# Otherwise, load the nltk stopwords list in English\n\n# Create an empty Python list to hold the stopwords\nstop_words = []\n\n# The filename of the custom data/stop_words.csv file\nstopwords_list_filename = 'data/stop_words.csv'\n\nif os.path.exists(stopwords_list_filename):\n    import csv\n    with open(stopwords_list_filename, 'r') as f:\n        stop_words = list(csv.reader(f))[0]\n    print('Custom stopwords list loaded from CSV')\nelse:\n    # Load the NLTK stopwords list\n    from nltk.corpus import stopwords\n    stop_words = stopwords.words('english')\n    print('NLTK stopwords list loaded')","execution_count":4,"outputs":[{"output_type":"stream","text":"NLTK stopwords list loaded\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Define a Unigram Processing Function\nIn this step, we gather the unigrams. If there is a Pre-Processing Filter, we will only analyze documents from the filtered ID list. We will also process each unigram, assessing them individually. We will complete the following tasks:\n\n* Lowercase all tokens\n* Remove tokens in stopwords list\n* Remove tokens with fewer than 4 characters\n* Remove tokens with non-alphabetic characters\n\nWe can define this process in a function."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a function that will process individual tokens\n# Only a token that passes through all three `if` \n# statements will be returned. A `True` result for\n# any `if` statement does not return the token. \n\nimport string\n\ndef process_token(token):\n    token = token.lower()\n    if token in stop_words: # If True, do not return token\n        return\n    token = token.strip(string.punctuation)\n    if len(token) < 4: # If True, do not return token\n        return\n    if not(token.isalpha()): # If True, do not return token\n        return\n    return token # If all are False, return the lowercased token","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'blue-green'.isalpha()","execution_count":79,"outputs":[{"output_type":"execute_result","execution_count":79,"data":{"text/plain":"False"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Collect lists of Document IDs, Titles, and Unigrams\n\nNext, we process all the unigrams into a list called `documents`. For demonstration purposes, this code runs on a limit of 500 documents, but we can change this to process all the documents. We are also collecting the document titles and ids so we can reference them later."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Collecting the unigrams and processing them into `documents`\n\nlimit = 10000 # Change number of documents being analyzed. Set to `None` to do all documents.\nn = 0\ndocuments = []\ndocument_ids = []\ndocument_titles = []\n    \nfor document in tdm_client.dataset_reader(dataset_file):\n    processed_document = []\n    document_id = document['id']\n    document_title = document['title']\n    if use_filtered_list is True:\n        # Skip documents not in our filtered_id_list\n        if document_id not in filtered_id_list:\n            continue\n    document_ids.append(document_id)\n    document_titles.append(document_title) \n    unigrams = document.get(\"unigramCount\", [])\n    for gram, count in unigrams.items():\n        clean_gram = process_token(gram)\n        if clean_gram is None:\n            continue\n        processed_document.append(clean_gram)\n    if len(processed_document) > 0:\n        documents.append(processed_document)\n    n += 1\n    if (limit is not None) and (n >= limit):\n        break\nprint('Unigrams collected and processed.')","execution_count":53,"outputs":[{"output_type":"stream","text":"Unigrams collected and processed.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"At this point, we have unigrams collected for all our documents insde the `documents` list variable. Each index of our list is a single document, starting with `documents[0]`. Each document is, in turn, a list with a single stringe for each unigram.\n\n**Note:** As we collect the unigrams for each document, we are simply including them in a list of strings. This is not the same as collecting them into word counts, and we are not using a Counter() object here like the Word Frequencies notebook. \n\nThe next cell demonstrates the contents of each item in our `document` list. Essentially, "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Show the unigrams collected for a particular document\ndocuments[0]","execution_count":54,"outputs":[{"output_type":"execute_result","execution_count":54,"data":{"text/plain":"['graph',\n 'fact',\n 'using',\n 'myself',\n 'rely',\n 'acts',\n 'isomorphic',\n 'points',\n 'chapter',\n 'comes',\n 'proof',\n 'domain',\n 'actually',\n 'matrices',\n 'referring',\n 'state',\n 'discrete',\n 'method',\n 'delay',\n 'focus',\n 'result',\n 'outline',\n 'previous',\n 'graham',\n 'many',\n 'product',\n 'many',\n 'also',\n 'hence',\n 'prefer',\n 'rational',\n 'binary',\n 'prove',\n 'power',\n 'proper',\n 'previously',\n 'terminology',\n 'form',\n 'generality',\n 'isomorphism',\n 'graph',\n 'interval',\n 'express',\n 'following',\n 'includes',\n 'small',\n 'baumslag',\n 'used',\n 'known',\n 'image',\n 'unlike',\n 'free',\n 'closed',\n 'basis',\n 'acting',\n 'slightly',\n 'considered',\n 'solitar',\n 'expressed',\n 'however',\n 'therefore',\n 'otherwise',\n 'neither',\n 'form',\n 'prove',\n 'useful',\n 'rationals',\n 'like',\n 'takes',\n 'groups',\n 'space',\n 'given',\n 'orbit',\n 'endpoint',\n 'sees',\n 'every',\n 'line',\n 'exercises',\n 'function',\n 'homomorphism',\n 'quotation',\n 'proposition',\n 'doomed',\n 'attributed',\n 'freely',\n 'elements',\n 'this',\n 'proof',\n 'group',\n 'integer',\n 'cayley',\n 'developed',\n 'infinite',\n 'remark',\n 'trick',\n 'mathematical',\n 'intersects',\n 'operation',\n 'anyway',\n 'remainder',\n 'provides',\n 'integer',\n 'well',\n 'referred',\n 'positive',\n 'tools',\n 'outline',\n 'showed',\n 'inverse',\n 'considered',\n 'fact',\n 'example',\n 'numbers',\n 'denoted',\n 'example',\n 'paper',\n 'contains',\n 'again',\n 'transitive',\n 'integers',\n 'length',\n 'wrote',\n 'different',\n 'chapter',\n 'know',\n 'viewed',\n 'smallest',\n 'identity',\n 'finite',\n 'construction',\n 'general',\n 'products',\n 'case',\n 'greater',\n 'entirely',\n 'where',\n 'corollary',\n 'acts',\n 'around',\n 'group',\n 'notice',\n 'tions',\n 'stabilizer',\n 'drawing',\n 'text',\n 'respect',\n 'line',\n 'groups',\n 'instead',\n 'either',\n 'topological',\n 'further',\n 'infinitely',\n 'others',\n 'point',\n 'coefficient',\n 'arguing',\n 'possible',\n 'linear',\n 'time',\n 'draw',\n 'explicit',\n 'section',\n 'gilbert',\n 'repeat',\n 'introduce',\n 'proper',\n 'often',\n 'exercise',\n 'finitely',\n 'indicates',\n 'generators',\n 'steps',\n 'interval',\n 'least',\n 'fundamental',\n 'generated',\n 'action',\n 'vigorous',\n 'otherwise',\n 'functions',\n 'complain',\n 'described',\n 'neighborhood',\n 'hence',\n 'higman',\n 'consider',\n 'actually',\n 'reflections',\n 'fill',\n 'give',\n 'sustained',\n 'chapters',\n 'generator',\n 'analog',\n 'vertices',\n 'rephrase',\n 'linear',\n 'commonly',\n 'prescribed',\n 'waged',\n 'composition',\n 'show',\n 'rationals',\n 'alternative',\n 'positive',\n 'asks',\n 'terms',\n 'presentation',\n 'donald',\n 'rational',\n 'exercise',\n 'verify',\n 'nicely',\n 'start',\n 'constructing',\n 'subgroup',\n 'first',\n 'abelian',\n 'discrete',\n 'worked',\n 'proving',\n 'finally',\n 'london',\n 'translation',\n 'real',\n 'proper',\n 'relations',\n 'subgroup',\n 'moved',\n 'discrete',\n 'since',\n 'reference',\n 'campaign',\n 'dyadic',\n 'cyclic',\n 'composed',\n 'thus',\n 'determined',\n 'proof',\n 'description',\n 'maps',\n 'normal',\n 'ultimately',\n 'seem',\n 'theorem',\n 'understand',\n 'importance',\n 'discussing',\n 'reason',\n 'journal',\n 'element',\n 'surjective',\n 'actions',\n 'dihedral',\n 'reader',\n 'left',\n 'form',\n 'composition',\n 'theory',\n 'hint',\n 'state',\n 'width',\n 'show',\n 'work',\n 'family',\n 'since',\n 'forms',\n 'functions',\n 'induction',\n 'integers',\n 'society',\n 'geometric',\n 'thus',\n 'unit',\n 'graphs',\n 'prove',\n 'order',\n 'defined',\n 'within',\n 'bijection',\n 'subset',\n 'object',\n 'follows',\n 'intervals']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"If we wanted to see word frequencies, we could convert the lists at this point into `Counter()` objects. The next cell demonstrates that operation."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Convert a given document into a Counter object to determine\n# word frequencies count\n\n# Import counter to help count word frequencies\nfrom collections import Counter\n\nword_freq = Counter(documents[0]) # Change documents index to see a different document\nword_freq.most_common(25) ","execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"[('proof', 3),\n ('discrete', 3),\n ('prove', 3),\n ('proper', 3),\n ('form', 3),\n ('graph', 2),\n ('fact', 2),\n ('acts', 2),\n ('chapter', 2),\n ('actually', 2),\n ('state', 2),\n ('outline', 2),\n ('many', 2),\n ('hence', 2),\n ('rational', 2),\n ('interval', 2),\n ('considered', 2),\n ('otherwise', 2),\n ('rationals', 2),\n ('groups', 2),\n ('line', 2),\n ('group', 2),\n ('integer', 2),\n ('positive', 2),\n ('example', 2)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now that we have all the cleaned unigrams for every document in a list called `documents`, we can use Gensim to compute TF/IDF."},{"metadata":{},"cell_type":"markdown","source":"---\n## Using Gensim to Compute \"Term Frequency- Inverse Document Frequency\"\n\nIt will be helpful to remember the basic steps we did in the explanatory [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) example:\n\n1. Create a list of the frequency of every word in every document\n2. Create a list of every word in the [corpus](https://docs.constellate.org/key-terms/#corpus)\n3. Compute [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) based on that data\n\nSo far, we have completed the first item by creating a list of the frequency of every word in every document. Now we need to create a list of every word in the corpus. In [gensim](https://docs.constellate.org/key-terms/#gensim), this is called a \"dictionary\". A [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary) is similar to a [Python dictionary](https://docs.constellate.org/key-terms/#python-dictionary), but here it is called a [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary) to show it is a specialized kind of dictionary.\n\n### Creating a Gensim Dictionary\n\nLet's create our [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary). A [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary) is a kind of masterlist of all the words across all the documents in our corpus. Each unique word is assigned an ID in the gensim dictionary. The result is a set of key/value pairs of unique tokens and their unique IDs."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import gensim\ndictionary = gensim.corpora.Dictionary(documents)","execution_count":74,"outputs":[{"output_type":"stream","text":"INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\nINFO:gensim.corpora.dictionary:built Dictionary(43912 unique tokens: ['abelian', 'acting', 'action', 'actions', 'acts']...) from 634 documents (total 722668 corpus positions)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Now that we have a [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary), we can get a preview that displays the number of unique tokens across all of our texts."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(dictionary)","execution_count":75,"outputs":[{"output_type":"stream","text":"Dictionary(43912 unique tokens: ['abelian', 'acting', 'action', 'actions', 'acts']...)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary) stores a unique identifier (starting with 0) for every unique token in the corpus. The [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary) does not contain information on word frequencies; it only catalogs all the unique words in the corpus. You can see the unique ID for each token in the text using the .token2id() method."},{"metadata":{"trusted":true},"cell_type":"code","source":"list(dictionary.token2id.items())","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"[('abelian', 0),\n ('acting', 1),\n ('action', 2),\n ('actions', 3),\n ('acts', 4),\n ('actually', 5),\n ('again', 6),\n ('also', 7),\n ('alternative', 8),\n ('analog', 9),\n ('anyway', 10),\n ('arguing', 11),\n ('around', 12),\n ('asks', 13),\n ('attributed', 14),\n ('basis', 15),\n ('baumslag', 16),\n ('bijection', 17),\n ('binary', 18),\n ('campaign', 19),\n ('case', 20),\n ('cayley', 21),\n ('chapter', 22),\n ('chapters', 23),\n ('closed', 24),\n ('coefficient', 25),\n ('comes', 26),\n ('commonly', 27),\n ('complain', 28),\n ('composed', 29),\n ('composition', 30),\n ('consider', 31),\n ('considered', 32),\n ('constructing', 33),\n ('construction', 34),\n ('contains', 35),\n ('corollary', 36),\n ('cyclic', 37),\n ('defined', 38),\n ('delay', 39),\n ('denoted', 40),\n ('described', 41),\n ('description', 42),\n ('determined', 43),\n ('developed', 44),\n ('different', 45),\n ('dihedral', 46),\n ('discrete', 47),\n ('discussing', 48),\n ('domain', 49),\n ('donald', 50),\n ('doomed', 51),\n ('draw', 52),\n ('drawing', 53),\n ('dyadic', 54),\n ('either', 55),\n ('element', 56),\n ('elements', 57),\n ('endpoint', 58),\n ('entirely', 59),\n ('every', 60),\n ('example', 61),\n ('exercise', 62),\n ('exercises', 63),\n ('explicit', 64),\n ('express', 65),\n ('expressed', 66),\n ('fact', 67),\n ('family', 68),\n ('fill', 69),\n ('finally', 70),\n ('finite', 71),\n ('finitely', 72),\n ('first', 73),\n ('focus', 74),\n ('following', 75),\n ('follows', 76),\n ('form', 77),\n ('forms', 78),\n ('free', 79),\n ('freely', 80),\n ('function', 81),\n ('functions', 82),\n ('fundamental', 83),\n ('further', 84),\n ('general', 85),\n ('generality', 86),\n ('generated', 87),\n ('generator', 88),\n ('generators', 89),\n ('geometric', 90),\n ('gilbert', 91),\n ('give', 92),\n ('given', 93),\n ('graham', 94),\n ('graph', 95),\n ('graphs', 96),\n ('greater', 97),\n ('group', 98),\n ('groups', 99),\n ('hence', 100),\n ('higman', 101),\n ('hint', 102),\n ('homomorphism', 103),\n ('however', 104),\n ('identity', 105),\n ('image', 106),\n ('importance', 107),\n ('includes', 108),\n ('indicates', 109),\n ('induction', 110),\n ('infinite', 111),\n ('infinitely', 112),\n ('instead', 113),\n ('integer', 114),\n ('integers', 115),\n ('intersects', 116),\n ('interval', 117),\n ('intervals', 118),\n ('introduce', 119),\n ('inverse', 120),\n ('isomorphic', 121),\n ('isomorphism', 122),\n ('journal', 123),\n ('know', 124),\n ('known', 125),\n ('least', 126),\n ('left', 127),\n ('length', 128),\n ('like', 129),\n ('line', 130),\n ('linear', 131),\n ('london', 132),\n ('many', 133),\n ('maps', 134),\n ('mathematical', 135),\n ('matrices', 136),\n ('method', 137),\n ('moved', 138),\n ('myself', 139),\n ('neighborhood', 140),\n ('neither', 141),\n ('nicely', 142),\n ('normal', 143),\n ('notice', 144),\n ('numbers', 145),\n ('object', 146),\n ('often', 147),\n ('operation', 148),\n ('orbit', 149),\n ('order', 150),\n ('others', 151),\n ('otherwise', 152),\n ('outline', 153),\n ('paper', 154),\n ('point', 155),\n ('points', 156),\n ('positive', 157),\n ('possible', 158),\n ('power', 159),\n ('prefer', 160),\n ('prescribed', 161),\n ('presentation', 162),\n ('previous', 163),\n ('previously', 164),\n ('product', 165),\n ('products', 166),\n ('proof', 167),\n ('proper', 168),\n ('proposition', 169),\n ('prove', 170),\n ('provides', 171),\n ('proving', 172),\n ('quotation', 173),\n ('rational', 174),\n ('rationals', 175),\n ('reader', 176),\n ('real', 177),\n ('reason', 178),\n ('reference', 179),\n ('referred', 180),\n ('referring', 181),\n ('reflections', 182),\n ('relations', 183),\n ('rely', 184),\n ('remainder', 185),\n ('remark', 186),\n ('repeat', 187),\n ('rephrase', 188),\n ('respect', 189),\n ('result', 190),\n ('section', 191),\n ('seem', 192),\n ('sees', 193),\n ('show', 194),\n ('showed', 195),\n ('since', 196),\n ('slightly', 197),\n ('small', 198),\n ('smallest', 199),\n ('society', 200),\n ('solitar', 201),\n ('space', 202),\n ('stabilizer', 203),\n ('start', 204),\n ('state', 205),\n ('steps', 206),\n ('subgroup', 207),\n ('subset', 208),\n ('surjective', 209),\n ('sustained', 210),\n ('takes', 211),\n ('terminology', 212),\n ('terms', 213),\n ('text', 214),\n ('theorem', 215),\n ('theory', 216),\n ('therefore', 217),\n ('this', 218),\n ('thus', 219),\n ('time', 220),\n ('tions', 221),\n ('tools', 222),\n ('topological', 223),\n ('transitive', 224),\n ('translation', 225),\n ('trick', 226),\n ('ultimately', 227),\n ('understand', 228),\n ('unit', 229),\n ('unlike', 230),\n ('used', 231),\n ('useful', 232),\n ('using', 233),\n ('verify', 234),\n ('vertices', 235),\n ('viewed', 236),\n ('vigorous', 237),\n ('waged', 238),\n ('well', 239),\n ('where', 240),\n ('width', 241),\n ('within', 242),\n ('work', 243),\n ('worked', 244),\n ('wrote', 245),\n ('abloabll', 246),\n ('above', 247),\n ('abstract', 248),\n ('accomodate', 249),\n ('acknowledgement', 250),\n ('adapted', 251),\n ('additional', 252),\n ('address', 253),\n ('admit', 254),\n ('advisor', 255),\n ('algorithm', 256),\n ('allowing', 257),\n ('ally', 258),\n ('along', 259),\n ('american', 260),\n ('among', 261),\n ('analogous', 262),\n ('angle', 263),\n ('angles', 264),\n ('annulus', 265),\n ('another', 266),\n ('answer', 267),\n ('appear', 268),\n ('appears', 269),\n ('applications', 270),\n ('applied', 271),\n ('apply', 272),\n ('appropriate', 273),\n ('april', 274),\n ('arbitrary', 275),\n ('arrow', 276),\n ('arrows', 277),\n ('asked', 278),\n ('aspherical', 279),\n ('associated', 280),\n ('assume', 281),\n ('assumed', 282),\n ('automatic', 283),\n ('available', 284),\n ('baloball', 285),\n ('basepoint', 286),\n ('beginning', 287),\n ('below', 288),\n ('bieri', 289),\n ('black', 290),\n ('boundary', 291),\n ('bridson', 292),\n ('build', 293),\n ('bull', 294),\n ('canberra', 295),\n ('cancellation', 296),\n ('cases', 297),\n ('center', 298),\n ('centers', 299),\n ('certain', 300),\n ('check', 301),\n ('choice', 302),\n ('choices', 303),\n ('choose', 304),\n ('chooses', 305),\n ('chopped', 306),\n ('chose', 307),\n ('class', 308),\n ('classification', 309),\n ('coefficients', 310),\n ('coher', 311),\n ('coherent', 312),\n ('cohomological', 313),\n ('college', 314),\n ('combinatorial', 315),\n ('comments', 316),\n ('communicated', 317),\n ('compact', 318),\n ('complex', 319),\n ('complexes', 320),\n ('complicated', 321),\n ('condition', 322),\n ('conditions', 323),\n ('conf', 324),\n ('conjugates', 325),\n ('conjugation', 326),\n ('connected', 327),\n ('consisting', 328),\n ('construct', 329),\n ('constructed', 330),\n ('containing', 331),\n ('cornell', 332),\n ('corner', 333),\n ('corners', 334),\n ('correspond', 335),\n ('corresponding', 336),\n ('corresponds', 337),\n ('corrrespond', 338),\n ('could', 339),\n ('cover', 340),\n ('current', 341),\n ('curvature', 342),\n ('curved', 343),\n ('cycle', 344),\n ('cycles', 345),\n ('daniel', 346),\n ('daniwisefnath', 347),\n ('decide', 348),\n ('decides', 349),\n ('define', 350),\n ('definition', 351),\n ('dehn', 352),\n ('demand', 353),\n ('denote', 354),\n ('department', 355),\n ('derived', 356),\n ('designed', 357),\n ('desired', 358),\n ('diagram', 359),\n ('diagrams', 360),\n ('differs', 361),\n ('difficult', 362),\n ('dimension', 363),\n ('direct', 364),\n ('distinct', 365),\n ('divided', 366),\n ('easier', 367),\n ('easily', 368),\n ('easy', 369),\n ('edge', 370),\n ('edges', 371),\n ('editors', 372),\n ('eliminating', 373),\n ('empty', 374),\n ('encouragement', 375),\n ('endomorphism', 376),\n ('endpoints', 377),\n ('enough', 378),\n ('equation', 379),\n ('equivalent', 380),\n ('essays', 381),\n ('essential', 382),\n ('even', 383),\n ('exact', 384),\n ('exactly', 385),\n ('examples', 386),\n ('exceeds', 387),\n ('exhibiting', 388),\n ('exists', 389),\n ('exposition', 390),\n ('extended', 391),\n ('extension', 392),\n ('fashion', 393),\n ('figure', 394),\n ('figures', 395),\n ('find', 396),\n ('finding', 397),\n ('fine', 398),\n ('followed', 399),\n ('formed', 400),\n ('furthermore', 401),\n ('generalized', 402),\n ('generally', 403),\n ('generates', 404),\n ('gersten', 405),\n ('gluing', 406),\n ('grateful', 407),\n ('greatly', 408),\n ('grey', 409),\n ('gromov', 410),\n ('grows', 411),\n ('haefliger', 412),\n ('hall', 413),\n ('helpful', 414),\n ('heptagon', 415),\n ('homological', 416),\n ('hope', 417),\n ('hyperbolic', 418),\n ('idea', 419),\n ('iisji', 420),\n ('immediately', 421),\n ('improved', 422),\n ('incident', 423),\n ('incoherent', 424),\n ('indeed', 425),\n ('index', 426),\n ('induces', 427),\n ('instance', 428),\n ('intersection', 429),\n ('introduction', 430),\n ('inverses', 431),\n ('involve', 432),\n ('ithaca', 433),\n ('joined', 434),\n ('kernel', 435),\n ('label', 436),\n ('labeled', 437),\n ('labelled', 438),\n ('labels', 439),\n ('language', 440),\n ('large', 441),\n ('lecture', 442),\n ('lemma', 443),\n ('less', 444),\n ('letters', 445),\n ('lift', 446),\n ('lifting', 447),\n ('lifts', 448),\n ('light', 449),\n ('link', 450),\n ('long', 451),\n ('lower', 452),\n ('lyndon', 453),\n ('main', 454),\n ('makes', 455),\n ('making', 456),\n ('manner', 457),\n ('mapped', 458),\n ('march', 459),\n ('mary', 460),\n ('math', 461),\n ('mathematics', 462),\n ('means', 463),\n ('meant', 464),\n ('meeting', 465),\n ('mentioned', 466),\n ('mentioning', 467),\n ('methods', 468),\n ('metric', 469),\n ('metrics', 470),\n ('metrize', 471),\n ('miller', 472),\n ('modified', 473),\n ('monoid', 474),\n ('motivate', 475),\n ('motivated', 476),\n ('must', 477),\n ('natural', 478),\n ('nearly', 479),\n ('need', 480),\n ('nega', 481),\n ('negative', 482),\n ('negatively', 483),\n ('nilpotent', 484),\n ('nonpositive', 485),\n ('notation', 486),\n ('note', 487),\n ('notes', 488),\n ('number', 489),\n ('objective', 490),\n ('observe', 491),\n ('obtain', 492),\n ('obtained', 493),\n ('obtaining', 494),\n ('obvious', 495),\n ('obviously', 496),\n ('offers', 497),\n ('orient', 498),\n ('pact', 499),\n ('pages', 500),\n ('pair', 501),\n ('part', 502),\n ('particular', 503),\n ('parts', 504),\n ('path', 505),\n ('pathological', 506),\n ('pathologies', 507),\n ('pathology', 508),\n ('paths', 509),\n ('pentagon', 510),\n ('pentagons', 511),\n ('perbolic', 512),\n ('phabet', 513),\n ('pieces', 514),\n ('place', 515),\n ('pleasure', 516),\n ('plethora', 517),\n ('polygon', 518),\n ('polygons', 519),\n ('postpone', 520),\n ('preimage', 521),\n ('presentations', 522),\n ('presented', 523),\n ('primary', 524),\n ('princeton', 525),\n ('problem', 526),\n ('problems', 527),\n ('proc', 528),\n ('proceedings', 529),\n ('prod', 530),\n ('produce', 531),\n ('produces', 532),\n ('producing', 533),\n ('proofs', 534),\n ('property', 535),\n ('proved', 536),\n ('provide', 537),\n ('queen', 538),\n ('question', 539),\n ('questions', 540),\n ('quite', 541),\n ('quotient', 542),\n ('rank', 543),\n ('received', 544),\n ('recipe', 545),\n ('recursively', 546),\n ('reduced', 547),\n ('referees', 548),\n ('references', 549),\n ('regular', 550),\n ('relation', 551),\n ('relator', 552),\n ('relators', 553),\n ('remaining', 554),\n ('repeti', 555),\n ('repetitions', 556),\n ('resolved', 557),\n ('respectively', 558),\n ('responds', 559),\n ('restricted', 560),\n ('right', 561),\n ('rips', 562),\n ('ronald', 563),\n ('roseblade', 564),\n ('said', 565),\n ('satisfied', 566),\n ('satisfies', 567),\n ('satisfy', 568),\n ('schupp', 569),\n ('scott', 570),\n ('seminar', 571),\n ('sense', 572),\n ('sequence', 573),\n ('sequences', 574),\n ('serre', 575),\n ('shaded', 576),\n ('sheds', 577),\n ('short', 578),\n ('shown', 579),\n ('shows', 580),\n ('similar', 581),\n ('simple', 582),\n ('simplifies', 583),\n ('solomon', 584),\n ('solvable', 585),\n ('somewhat', 586),\n ('spaces', 587),\n ('special', 588),\n ('squared', 589),\n ('stance', 590),\n ('standard', 591),\n ('states', 592),\n ('step', 593),\n ('stricter', 594),\n ('struct', 595),\n ('subcomplex', 596),\n ('subdivide', 597),\n ('subdivides', 598),\n ('subgroups', 599),\n ('subject', 600),\n ('submodule', 601),\n ('substantially', 602),\n ('substitution', 603),\n ('subword', 604),\n ('sufficient', 605),\n ('suggest', 606),\n ('suggested', 607),\n ('suitable', 608),\n ('supplied', 609),\n ('sure', 610),\n ('teger', 611),\n ('tempted', 612),\n ('thank', 613),\n ('that', 614),\n ('though', 615),\n ('tietze', 616),\n ('tively', 617),\n ('together', 618),\n ('torsion', 619),\n ('total', 620),\n ('transformation', 621),\n ('transparent', 622),\n ('triangle', 623),\n ('trivial', 624),\n ('true', 625),\n ('type', 626),\n ('understood', 627),\n ('university', 628),\n ('unsolvable', 629),\n ('uses', 630),\n ('usually', 631),\n ('various', 632),\n ('verified', 633),\n ('vertex', 634),\n ('volume', 635),\n ('whether', 636),\n ('white', 637),\n ('whose', 638),\n ('wise', 639),\n ('wish', 640),\n ('without', 641),\n ('word', 642),\n ('words', 643),\n ('worth', 644),\n ('would', 645),\n ('xjiixjiixj', 646),\n ('xjxh', 647),\n ('xlxj', 648),\n ('yields', 649),\n ('york', 650),\n ('actualités', 651),\n ('additive', 652),\n ('adler', 653),\n ('advance', 654),\n ('akin', 655),\n ('algebra', 656),\n ('algebraic', 657),\n ('algebraically', 658),\n ('algebraization', 659),\n ('almost', 660),\n ('although', 661),\n ('always', 662),\n ('amer', 663),\n ('analysables', 664),\n ('analysis', 665),\n ('analytic', 666),\n ('anand', 667),\n ('annals', 668),\n ('announce', 669),\n ('antiderivative', 670),\n ('anyone', 671),\n ('applica', 672),\n ('approach', 673),\n ('approaches', 674),\n ('approx', 675),\n ('approximate', 676),\n ('approximation', 677),\n ('area', 678),\n ('areas', 679),\n ('arithmetic', 680),\n ('article', 681),\n ('articles', 682),\n ('aschenbrenner', 683),\n ('aspects', 684),\n ('assigned', 685),\n ('association', 686),\n ('asymptotic', 687),\n ('author', 688),\n ('authors', 689),\n ('background', 690),\n ('bases', 691),\n ('basic', 692),\n ('beautiful', 693),\n ('behaviour', 694),\n ('belongs', 695),\n ('benoist', 696),\n ('berkovich', 697),\n ('bertrand', 698),\n ('biextensions', 699),\n ('birthday', 700),\n ('bost', 701),\n ('bouscaren', 702),\n ('breadth', 703),\n ('broader', 704),\n ('builds', 705),\n ('bulletin', 706),\n ('burgeoning', 707),\n ('capacity', 708),\n ('cardinalities', 709),\n ('cardinality', 710),\n ('careful', 711),\n ('central', 712),\n ('characterisation', 713),\n ('characterising', 714),\n ('characteristic', 715),\n ('cherlin', 716),\n ('close', 717),\n ('coarse', 718),\n ('coauthors', 719),\n ('collection', 720),\n ('combinatorics', 721),\n ('comment', 722),\n ('compositional', 723),\n ('concepts', 724),\n ('concern', 725),\n ('concerns', 726),\n ('concludes', 727),\n ('conference', 728),\n ('conjecture', 729),\n ('conjectures', 730),\n ('connections', 731),\n ('consequence', 732),\n ('consequences', 733),\n ('consists', 734),\n ('constructive', 735),\n ('contained', 736),\n ('context', 737),\n ('contributions', 738),\n ('convex', 739),\n ('convey', 740),\n ('counting', 741),\n ('culture', 742),\n ('dahn', 743),\n ('dame', 744),\n ('decreasing', 745),\n ('definability', 746),\n ('definable', 747),\n ('degree', 748),\n ('denotes', 749),\n ('dependent', 750),\n ('derivatives', 751),\n ('describes', 752),\n ('developments', 753),\n ('devised', 754),\n ('differential', 755),\n ('differentiation', 756),\n ('dimen', 757),\n ('dimensions', 758),\n ('diophantine', 759),\n ('direction', 760),\n ('directions', 761),\n ('discuss', 762),\n ('disjoint', 763),\n ('diversity', 764),\n ('dries', 765),\n ('duced', 766),\n ('dugald', 767),\n ('duke', 768),\n ('dulac', 769),\n ('earlier', 770),\n ('edited', 771),\n ('elaborations', 772),\n ('ellip', 773),\n ('else', 774),\n ('energy', 775),\n ('equations', 776),\n ('erdös', 777),\n ('essentially', 778),\n ('everywhere', 779),\n ('expansions', 780),\n ('explicitly', 781),\n ('explored', 782),\n ('explores', 783),\n ('exponential', 784),\n ('exponentiation', 785),\n ('expository', 786),\n ('extends', 787),\n ('field', 788),\n ('fields', 789),\n ('final', 790),\n ('finitary', 791),\n ('fitting', 792),\n ('five', 793),\n ('floors', 794),\n ('fonctions', 795),\n ('formal', 796),\n ('formula', 797),\n ('formulates', 798),\n ('four', 799),\n ('framework', 800),\n ('frameworks', 801),\n ('france', 802),\n ('gaga', 803),\n ('generalisation', 804),\n ('generalisations', 805),\n ('generic', 806),\n ('generically', 807),\n ('geometries', 808),\n ('geometry', 809),\n ('gives', 810),\n ('goal', 811),\n ('goring', 812),\n ('hand', 813),\n ('handling', 814),\n ('heavily', 815),\n ('height', 816),\n ('held', 817),\n ('hermann', 818),\n ('highly', 819),\n ('hoeven', 820),\n ('homogeneous', 821),\n ('honour', 822),\n ('hrushovski', 823),\n ('hull', 824),\n ('ideas', 825),\n ('imate', 826),\n ('important', 827),\n ('including', 828),\n ('independence', 829),\n ('independent', 830),\n ('independently', 831),\n ('inequality', 832),\n ('influence', 833),\n ('influential', 834),\n ('initially', 835),\n ('insightful', 836),\n ('inspirational', 837),\n ('inspired', 838),\n ('inspiring', 839),\n ('interactions', 840),\n ('interconnections', 841),\n ('interesting', 842),\n ('interpretations', 843),\n ('intersections', 844),\n ('intro', 845),\n ('irreducible', 846),\n ('june', 847),\n ('keisler', 848),\n ('kind', 849),\n ('largely', 850),\n ('larsen', 851),\n ('later', 852),\n ('launch', 853),\n ('leeds', 854),\n ('library', 855),\n ('likely', 856),\n ('linking', 857),\n ('links', 858),\n ('loeser', 859),\n ('logic', 860),\n ('louder', 861),\n ('macintyre', 862),\n ('macpherson', 863),\n ('made', 864),\n ('major', 865),\n ('marker', 866),\n ('masser', 867),\n ('mathématiques', 868),\n ('maximal', 869),\n ('measure', 870),\n ('measures', 871),\n ('ment', 872),\n ('ments', 873),\n ('mixed', 874),\n ('mixing', 875),\n ('model', 876),\n ('modular', 877),\n ('modularity', 878),\n ('much', 879),\n ('multiplication', 880),\n ('namely', 881),\n ('nected', 882),\n ('nonabelian', 883),\n ('nonstandard', 884),\n ('nonzero', 885),\n ('notion', 886),\n ('notions', 887),\n ('notre', 888),\n ('obviate', 889),\n ('oleron', 890),\n ('omitting', 891),\n ('opening', 892),\n ('ordered', 893),\n ('ordering', 894),\n ('overview', 895),\n ('painlevé', 896),\n ('papers', 897),\n ('paris', 898),\n ('participants', 899),\n ('partly', 900),\n ('perin', 901),\n ('period', 902),\n ('pervasiveness', 903),\n ('peterzil', 904),\n ('pila', 905),\n ('pillay', 906),\n ('pink', 907),\n ('playing', 908),\n ('poincaré', 909),\n ('possibility', 910),\n ('potentially', 911),\n ('powers', 912),\n ('preliminary', 913),\n ('preuve', 914),\n ('probability', 915),\n ('progress', 916),\n ('properties', 917),\n ('proposed', 918),\n ('proposes', 919),\n ('proposing', 920),\n ('proves', 921),\n ('pseudofinite', 922),\n ('publication', 923),\n ('pure', 924),\n ('quasifinite', 925),\n ('range', 926),\n ('ranging', 927),\n ('rather', 928),\n ('realisations', 929),\n ('realised', 930),\n ('reals', 931),\n ('recent', 932),\n ('recently', 933),\n ('reflects', 934),\n ('related', 935),\n ('relating', 936),\n ('relationship', 937),\n ('relevant', 938),\n ('reproved', 939),\n ('results', 940),\n ('reverse', 941),\n ('review', 942),\n ('revisiting', 943),\n ('revisits', 944),\n ('rich', 945),\n ('role', 946),\n ('rössler', 947),\n ('sample', 948),\n ('schemes', 949),\n ('school', 950),\n ('scope', 951),\n ('segment', 952),\n ('sela', 953),\n ('selected', 954),\n ('semiabelian', 955),\n ('semialgebraic', 956),\n ('september', 957),\n ('sets', 958),\n ('setting', 959),\n ('several', 960),\n ('sfin', 961),\n ('shelf', 962),\n ('shimura', 963),\n ('significant', 964),\n ('sions', 965),\n ('sixtieth', 966),\n ('sklinos', 967),\n ('slowly', 968),\n ('smooth', 969),\n ('solution', 970),\n ('sophisticated', 971),\n ('spirit', 972),\n ('stability', 973),\n ('stable', 974),\n ('starchenko', 975),\n ('statistically', 976),\n ('stems', 977),\n ('strengthening', 978),\n ('stronger', 979),\n ('strongly', 980),\n ('structures', 981),\n ('students', 982),\n ('studies', 983),\n ('subsets', 984),\n ('suggestions', 985),\n ('sums', 986),\n ('superstability', 987),\n ('supervised', 988),\n ('supply', 989),\n ('support', 990),\n ('surfaces', 991),\n ('survey', 992),\n ('symbolic', 993),\n ('take', 994),\n ('taking', 995),\n ('talks', 996),\n ('tantalising', 997),\n ('testament', 998),\n ('themes', 999),\n ...]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We could also look up the corresponding ID for a token using the ``.get`` method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the value for the key 'people'. Return 0 if there is no token matching 'people'. \n# The number returned is the gensim dictionary ID for the token. \n\ndictionary.token2id.get('hyperbolic', 'None') ","execution_count":80,"outputs":[{"output_type":"execute_result","execution_count":80,"data":{"text/plain":"418"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"For the sake of example, we could also discover a particular token using just the ID number. This is not something likely to happen in practice, but it serves here as a demonstration of the connection between tokens and their ID number.\n\nNormally, [Python dictionaries](https://docs.constellate.org/key-terms/#python-dictionary) only map from keys to values (not from values to keys). However, we can write a quick for loop to go the other direction. This cell is simply to demonstrate how the [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary) is connected to the list entries in the [gensim](https://docs.constellate.org/key-terms/#gensim) ``bow_corpus``."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the token associated with a token id number\ntoken_id = 17\n\n# If the token id matches, print out the associated token\nfor dict_id, token in dictionary.items():\n    if dict_id == token_id:\n        print(token)","execution_count":77,"outputs":[{"output_type":"stream","text":"bijection\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Creating a Bag of Words Corpus\n\nThe next step is to connect our word frequency data found within ``documents`` to our [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary) token IDs. For every document, we want to know how many times a word (notated by its ID) occurs. We will create a [Python list](https://docs.constellate.org/key-terms/#python-list) called ``bow_corpus`` that will turn our word counts into a series of [tuples](https://docs.constellate.org/key-terms/#tuple) where the first number is the [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary) token ID and the second number is the word frequency.\n\n![Combining Gensim dictionary with documents list to create Bag of Words Corpus](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/bag-of-words-creation.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a bag of words corpus\n\nbow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n\nprint('Bag of words corpus created successfully.')","execution_count":61,"outputs":[{"output_type":"stream","text":"Bag of words corpus created successfully.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examine the bag of words corpus for a specific document\n\nlist(bow_corpus[5][:10]) # List out a slice of the first ten items","execution_count":83,"outputs":[{"output_type":"execute_result","execution_count":83,"data":{"text/plain":"[(0, 3),\n (2, 1),\n (3, 2),\n (4, 1),\n (5, 1),\n (7, 1),\n (8, 1),\n (20, 2),\n (22, 1),\n (31, 1)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Using IDs can seem a little abstract, but we can discover the word associated with a particular ID. For demonstration purposes, the following code will replace the token IDs in the last example with the actual tokens."},{"metadata":{"trusted":true},"cell_type":"code","source":"word_counts = [[(dictionary[id], count) for id, count in line] for line in bow_corpus]\nlist(word_counts[5][:10])","execution_count":93,"outputs":[{"output_type":"execute_result","execution_count":93,"data":{"text/plain":"[('abelian', 3),\n ('action', 1),\n ('actions', 2),\n ('acts', 1),\n ('actually', 1),\n ('also', 1),\n ('alternative', 1),\n ('case', 2),\n ('chapter', 1),\n ('consider', 1)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Create the `TfidfModel`"},{"metadata":{},"cell_type":"markdown","source":"The next step is to create the [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) model which will set the parameters for our implementation of [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf). In our [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) example, the formula for [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) was:\n\n$$(Times-the-word-occurs-in-given-document) \\cdot \\mbox{log} \\frac{(Total-number-of-documents)}{(Number-of-documents-containing-word)}$$\n\nIn [gensim](https://docs.constellate.org/key-terms/#gensim), the default formula for measuring [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) uses log base 2 instead of log base 10, as shown:\n\n$$(Times-the-word-occurs-in-given-document) \\cdot \\log_{2} \\frac{(Total-number-of-documents)}{(Number-of-documents-containing-the-word)}$$\n\nIf you would like to use a different formula for your [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) calculation, there is a description of [parameters you can pass](https://radimrehurek.com/gensim/models/tfidfmodel.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create our gensim TF-IDF model\nmodel = gensim.models.TfidfModel(bow_corpus) ","execution_count":64,"outputs":[{"output_type":"stream","text":"INFO:gensim.models.tfidfmodel:collecting document frequencies\nINFO:gensim.models.tfidfmodel:PROGRESS: processing document #0\nINFO:gensim.models.tfidfmodel:calculating IDF weights for 634 documents and 43912 features (552236 matrix non-zeros)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Now, we apply our model to the ``bow_corpus`` to create our results in ``corpus_tfidf``. The ``corpus_tfidf`` is a python list of each document similar to ``bow_document``. Instead of listing the frequency next to the [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary) ID, however, it contains the [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) score for the associated token. Below, we display the first document in ``corpus_tfidf``."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Create TF-IDF scores for the ``bow_corpus`` using our model\n# Also create TF-IDF scores keyed off unigram string instead of gensim dictionary id\n\ncorpus_tfidf = model[bow_corpus]\nexample_tfidf_scores = [[(dictionary[id], count) for id, count in line] for line in corpus_tfidf]","execution_count":97,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List out the TF-IDF scores for the first 10 tokens of the first text in the corpus\nlist(corpus_tfidf[5][:10])","execution_count":98,"outputs":[{"output_type":"execute_result","execution_count":98,"data":{"text/plain":"[(0, 0.027287195813687024),\n (2, 0.006193903565093521),\n (3, 0.020086563594622107),\n (4, 0.008133639715420712),\n (5, 0.011504563435100174),\n (7, 0.0004264193484837412),\n (8, 0.017667706535831728),\n (20, 0.0016444891754621865),\n (22, 0.016030620320733695),\n (31, 0.0020857063030258974)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Let's display the tokens instead of the [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary) IDs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# List out the TF-IDF scores for the first 10 tokens of the first text in the corpus\nlist(example_tfidf_scores[5][:10]) ","execution_count":99,"outputs":[{"output_type":"execute_result","execution_count":99,"data":{"text/plain":"[('abelian', 0.027287195813687024),\n ('action', 0.006193903565093521),\n ('actions', 0.020086563594622107),\n ('acts', 0.008133639715420712),\n ('actually', 0.011504563435100174),\n ('also', 0.0004264193484837412),\n ('alternative', 0.017667706535831728),\n ('case', 0.0016444891754621865),\n ('chapter', 0.016030620320733695),\n ('consider', 0.0020857063030258974)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Find Top Terms in a Single Document\nFinally, let's sort the terms by their [TF-IDF](https://docs.constellate.org/key-terms/#tf-idf) weights to find the most significant terms in the document."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Sort the tuples in our tf-idf scores list\n\n# Choosing a document by its index number\n# Change n to see a different document\nn = 5\n\ndef Sort(tfidf_tuples): \n    tfidf_tuples.sort(key = lambda x: x[1], reverse=True) \n    return tfidf_tuples \n\n# Print the document id and title\nprint('Title: ', document_titles[n])\nprint('ID: ', document_ids[n])\n\n#List the top twenty tokens in our example document by their TF-IDF scores\nlist(Sort(example_tfidf_scores[n])[:20]) ","execution_count":102,"outputs":[{"output_type":"stream","text":"Title:  Homological Techniques for Strongly Graded Rings: A Survey\nID:  ark://27927/pbd6g0c40h\n","name":"stdout"},{"output_type":"execute_result","execution_count":102,"data":{"text/plain":"[('aljadeff', 0.17166508111260184),\n ('chouinard', 0.17166508111260184),\n ('ginosar', 0.17166508111260184),\n ('rxmodule', 0.17166508111260184),\n ('resolutions', 0.15890941803360442),\n ('evens', 0.14243508472272443),\n ('graded', 0.13733357409216954),\n ('rings', 0.1298546079795905),\n ('projectivity', 0.12884391039075085),\n ('cornick', 0.12399301030356452),\n ('moore', 0.10929106250837356),\n ('eckmann', 0.10786598575369791),\n ('tensoring', 0.10786598575369791),\n ('quinn', 0.10555093588440462),\n ('mislin', 0.10144955829986066),\n ('module', 0.10100663012125463),\n ('aljadeffand', 0.08583254055630092),\n ('comtek', 0.08583254055630092),\n ('dade', 0.08583254055630092),\n ('dimkpn', 0.08583254055630092)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We could also analyze across the entire corpus to find the most unique terms. These are terms that appear in a particular text, but rarely or never appear in other texts. (Often, these will be proper names since a particular article may mention a name often but the name may rarely appear in other articles. There's also a fairly good chance these will be typos or errors in optical character recognition.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a dictionary ``td`` where each document gather\ntd = { \ndictionary.get(_id): value for doc in corpus_tfidf\nfor _id, value in doc\n}\n\n# Sort the items of ``td`` into a new variable ``sorted_td``\n# the ``reverse`` starts from highest to lowest\nsorted_td = sorted(td.items(), key=lambda kv: kv[1], reverse=True) ","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for term, weight in sorted_td[:25]: # Print the top 25 terms in the entire corpus\n    print(term, weight)","execution_count":69,"outputs":[{"output_type":"stream","text":"divk 0.49389123037412147\ntmax 0.4639313507523522\npseudocharacters 0.4440562717163367\nhyperspherical 0.44214591896725747\nleafages 0.4367000988486141\nxiaoman 0.43140437956890487\nhyperbolization 0.42879725479274416\ncann 0.4163130878867191\nnakaoka 0.4113357107314544\nobraztsov 0.40852483782114507\nhemidiscrete 0.40186646565791384\nlasheras 0.39312064611316916\nantoniuk 0.3813156513482767\nscwols 0.38073238405578\nhillman 0.3797701539803256\npegs 0.3792514165752814\nsncx 0.3771170076222464\nenfngn 0.3765251699202019\nymnθ 0.3735617215149732\ncobracket 0.37199218225447134\nsubnegative 0.3655715945855486\nprotree 0.36217815560411404\nlandscapes 0.3588602549124525\nquasipolynomials 0.3556034971182082\ndismantlability 0.3554600581843127\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Display Most Significant Term for each Document\nWe can see the most significant term in every document."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# For each document, print the ID, most significant/unique word, and TF/IDF score\n\nn = 0\n\nfor n, doc in enumerate(corpus_tfidf):\n    if len(doc) < 1:\n        continue\n    word_id, score = max(doc, key=lambda x: x[1])\n    print(document_ids[n], dictionary.get(word_id), score)\n    if n >= 10:\n        break","execution_count":70,"outputs":[{"output_type":"stream","text":"ark://27927/pbd6j9j9ng waged 0.229536326662481\nhttp://www.jstor.org/stable/118602 incoherent 0.49416190715474884\nhttp://www.jstor.org/stable/23927282 transseries 0.2240610414704663\nark://27927/phx7x24r0rb chevie 0.19076751138165948\nark://27927/phx5wq0m2v3 robot 0.17128959234197738\nark://27927/pbd6g0c40h aljadeff 0.17166508111260184\nark://27927/phx4h1622s5 semistable 0.24202992966776743\nark://27927/phx4f4ks6tb dead 0.16471522883819062\nhttp://www.jstor.org/stable/3844991 exth 0.13967556367522585\nhttp://www.jstor.org/stable/20721717 higes 0.3257865465150734\nark://27927/pbd6fxfg01 spiky 0.29802887392527994\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Ranking documents by TF-IDF Score for a Search Word\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nterms_to_docs = defaultdict(list)\nfor doc_id, doc in enumerate(corpus_tfidf):\n    for term_id, value in doc:\n        term = dictionary.get(term_id)\n        terms_to_docs[term].append((doc_id, value))\n    if doc_id >= 500:\n        break\n","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick a unigram to discover its score across documents\nsearch_term = 'coriolanus'\n\n# Display a list of documents and scores for the search term\nmatching = terms_to_docs.get(search_term)\nfor doc_id, score in sorted(matching, key=lambda x: x[1], reverse=True):\n    print(document_ids[doc_id], score)","execution_count":72,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"'NoneType' object is not iterable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-72-5254fe1f7e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display a list of documents and scores for the search term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmatching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterms_to_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick a unigram to discover its score across documents\nsearch_term = 'hyperbolic'\n\n# Display a list of documents and scores for the search term\nmatching = terms_to_docs.get(search_term)\nfor doc_id, score in sorted(matching, key=lambda x: x[1], reverse=True):\n    print(document_ids[doc_id], score)","execution_count":73,"outputs":[{"output_type":"stream","text":"http://www.jstor.org/stable/4097604 0.07509385913233951\nhttp://www.jstor.org/stable/40067916 0.05192341649998301\nark://27927/phx4f4kr592 0.04839515731333261\nark://27927/phw4tcbxsf 0.040104965019452044\nhttp://www.jstor.org/stable/20752266 0.038485874941003265\nark://27927/phw4t4rrt1 0.037421455794926256\nark://27927/phx4f149zgc 0.03695599945551828\nark://27927/pgk26wgn37h 0.03615009584621512\nhttp://www.jstor.org/stable/117937 0.03562805588278146\nark://27927/pbd98f3bns 0.03521884569446371\nark://27927/phx4dk54q3z 0.03488696635507774\nark://27927/pbd6fxfg01 0.034560760077270436\nhttp://www.jstor.org/stable/24477626 0.03380584027898839\nark://27927/phx4h15t47j 0.032330890511672525\nark://27927/phzdsrqbnpp 0.03221739308450433\nhttp://www.jstor.org/stable/118035 0.03012414817562989\nark://27927/phz6cqq36b1 0.029440981557024737\nark://27927/phx4f4ks74t 0.02915478233105314\nark://27927/pghjmcv47z 0.028689542430261922\nark://27927/pgk1w0r2b27 0.028080602402389302\nark://27927/phzgkvwqvzd 0.027312268598735995\nhttp://www.jstor.org/stable/2162290 0.02730911030386044\nhttp://www.jstor.org/stable/119471 0.027118398669179592\nhttp://www.jstor.org/stable/24513027 0.027102357408604897\nark://27927/phzfvx773h3 0.026788018391060008\nhttp://www.jstor.org/stable/40039611 0.02649486714055518\nhttp://www.jstor.org/stable/4097962 0.025935987550219343\nark://27927/pgh3htm9077 0.025337753336125153\nark://27927/phx4dk5fb5q 0.025277509894484965\nark://27927/pgj2wbjs97 0.024915667214514624\nhttp://www.jstor.org/stable/43736926 0.024905168294390684\nark://27927/phz1ck64bdk 0.024099141673886864\nark://27927/pbd6ftwcdc 0.023880548910387593\nark://27927/phzd6rr0xqj 0.023777739106387513\nark://27927/phx65kdz3ct 0.023630869910641623\nark://27927/phx4dfzh51j 0.023577164953294635\nhttp://www.jstor.org/stable/23558856 0.02345934045734259\nark://27927/phz1dw9qbcp 0.02332613286163574\nark://27927/pbd1r35msdn 0.02320291797534355\nark://27927/pbd1r36jnrk 0.022818596363838286\nhttp://www.jstor.org/stable/40302733 0.022555351700629427\nhttp://www.jstor.org/stable/43736962 0.022261073583642637\nhttp://www.jstor.org/stable/23030567 0.021431766932953504\nark://27927/phz8tqst1kz 0.021287674203940815\nark://27927/pbd6fxfgd7 0.02127302536923615\nark://27927/pgjhwzzcth 0.021246350016379697\nark://27927/phx4f2c3zkp 0.021072788130486193\nhttp://www.jstor.org/stable/20161295 0.020927132401685467\nhttp://www.jstor.org/stable/117611 0.02085964338062499\nhttp://www.jstor.org/stable/43686105 0.020536893546728944\nhttp://www.jstor.org/stable/43679236 0.020488709704784534\nark://27927/phz122cpg6p 0.0199301122738539\nark://27927/phzfkwzbxhx 0.019926068819398294\nark://27927/phx4f5mbt3n 0.01975792339914975\nark://27927/pgh1918n5z8 0.01968016888109986\nark://27927/phzjgchv2rz 0.018907087511942856\nark://27927/phx4dm748k8 0.018741950357555943\nhttp://www.jstor.org/stable/23041858 0.01870474122575369\nark://27927/phz6mczk2nb 0.018472842398380627\nhttp://www.jstor.org/stable/43686517 0.018349743728617807\nark://27927/phx4dhz19tc 0.018294898065895997\nark://27927/phz30f0p6k5 0.018252963103875114\nark://27927/pbd6fmtd4b 0.018163881068070802\nhttp://www.jstor.org/stable/20161828 0.01814581241779658\nark://27927/phzhsggg946 0.018008298264661687\nark://27927/phx4f5vb3sz 0.017853472977177536\nark://27927/phx4f4kwnnr 0.017768756169219958\nark://27927/pgj9ff8ksq 0.017376181452168085\nark://27927/phx4dhzfgrn 0.017315465621962914\nark://27927/phzj7t940sc 0.017148984600727703\nark://27927/pgj9fm5drh 0.017114458304968973\nark://27927/pgk2zf50c59 0.017081118152171476\nhttp://www.jstor.org/stable/20721717 0.016778568313797197\nark://27927/phx4h164pb8 0.016735683944755334\nark://27927/phx6522rr7v 0.016684842392537876\nark://27927/pgjhwmzzs2 0.01658914739676687\nark://27927/pghbzvwq4k 0.01650429963064475\nhttp://www.jstor.org/stable/118602 0.0162194155554319\nhttp://www.jstor.org/stable/24488638 0.01608501815125975\nark://27927/pgk1d3dp0m1 0.015991740135670618\nark://27927/pbd69q1wjq 0.015965623469674145\nhttp://www.jstor.org/stable/23234624 0.01596329610194169\nark://27927/phx4kc4cmdq 0.015962173142504848\nhttp://www.jstor.org/stable/43302882 0.015722865911149905\nark://27927/pghbxf19ph 0.015590903298189062\nark://27927/pbd6fvsmpz 0.015383228045144356\nark://27927/phx4f1wf915 0.015371369972771241\nark://27927/pbd6g0c3kt 0.015316188303984912\nark://27927/phx4fq0tmfc 0.015273220197746929\nark://27927/pgj445q7gm 0.015194660202195304\nark://27927/phx4dwn7ddq 0.015192736038340624\nark://27927/phzc0331wr2 0.015077310810685323\nark://27927/phz8q442mqn 0.015002406659882945\nark://27927/pbd6j4df17 0.01492718860632464\nhttp://www.jstor.org/stable/3597318 0.014579744521688497\nark://27927/phz2htzx4pm 0.014534953940586993\nark://27927/phx4dk54knq 0.01434654738381351\nhttp://www.jstor.org/stable/117783 0.014259415604240065\nhttp://www.jstor.org/stable/117672 0.014140138247799151\nark://27927/phx8sfmp7h 0.01396352656440007\nark://27927/phzcdhg5mr9 0.013956504483044911\nark://27927/phx4f5v0b6c 0.013785373252595167\nark://27927/phx4h15s0gv 0.013610734799199756\nark://27927/pbd67xrvb8 0.013539550192997313\nhttp://www.jstor.org/stable/4097520 0.013525769123603239\nark://27927/pgh1gg8wgjz 0.013491325547615824\nark://27927/pbd1r36jq9f 0.013436568601989524\nark://27927/phz6pv3rknv 0.013317019664798906\nark://27927/pgj2z3zbbq 0.013310352332929806\nark://27927/phx7s0mqhhp 0.013259864709205904\nhttp://www.jstor.org/stable/41411052 0.013167101699115405\nhttp://www.jstor.org/stable/24493159 0.013156235758525205\nhttp://www.jstor.org/stable/23927282 0.012937988425613703\nhttp://www.jstor.org/stable/26409415 0.012837376412163098\nark://27927/phzcdhg5m1q 0.012655576461006264\nark://27927/phx4dfxrtrw 0.012652956488839549\nhttp://www.jstor.org/stable/24904185 0.012627230156734708\nark://27927/phx4djzq66z 0.012534438657880256\nark://27927/pbd6c8tmpm 0.012532223385276322\nark://27927/pbd999szbh 0.012377483630029279\nark://27927/pbd67xrtqg 0.012362797195411352\nark://27927/pbd68jt15t 0.012273820017671374\nark://27927/pbd6j4dk8s 0.012183795107413232\nhttp://www.jstor.org/stable/20535653 0.012021790468117226\nhttp://www.jstor.org/stable/3073045 0.012012180120843867\nark://27927/phx4f4m6gmd 0.011786986394212155\nhttp://www.jstor.org/stable/20161661 0.01169282904097237\nark://27927/phx4dhzfh24 0.011646557402001928\nark://27927/phz2vnxwwk4 0.011549055095873988\nark://27927/pbd1r36jpkb 0.01147539907292453\nark://27927/phx4gtg6xjw 0.011434816440321786\nark://27927/phz6d4p0whk 0.011417821488723024\nark://27927/phx4g1w82mw 0.011410883747034082\nark://27927/phz1drgsdrb 0.011325219631272688\nark://27927/phx2s0v6532 0.011309583366167305\nhttp://www.jstor.org/stable/23240564 0.011267747007388853\nark://27927/pbd1ph0n2hw 0.011243258425046553\nark://27927/phz8tqst2hc 0.011161429657298988\nark://27927/phx4d86n7sq 0.011009181808832301\nark://27927/phzdv2djx3s 0.010843615996418603\nark://27927/pbd1r36jqq2 0.010808380696129353\nhttp://www.jstor.org/stable/118653 0.010475595094307658\nark://27927/phz96nmwvzv 0.01033036742506894\nark://27927/phzjx4vdvp0 0.010317082120017518\nark://27927/pbd1r31zh67 0.010137453907609874\nark://27927/pgj2w9w232 0.010130672652092028\nark://27927/phx4f1wf86d 0.010047862602394224\nhttp://www.jstor.org/stable/23513361 0.010041254667619534\nark://27927/pbd6fvrndv 0.010019211715524825\nark://27927/phx4f1wf8m1 0.00999141513828319\nark://27927/pbd98f33w1 0.009972554397013294\nark://27927/pgh1g9c6sv6 0.00995553289424693\nhttp://www.jstor.org/stable/24513148 0.009899502381934927\nhttp://www.jstor.org/stable/20721733 0.009771269924578219\nark://27927/pbd1r34s19k 0.00967841683829012\nark://27927/phz5zsxqxk2 0.009504942316703359\nark://27927/pgkvs8nb7j 0.009473892701912547\nark://27927/phx4fdbst2p 0.009434788678744808\nhttp://www.jstor.org/stable/3647763 0.009185263512851405\nhttp://www.jstor.org/stable/40587257 0.008979135856443995\nark://27927/phx4fd8kfhc 0.008963049827333211\nark://27927/pgj2q4nwd6 0.00879489686583331\nhttp://www.jstor.org/stable/2646152 0.008733380814352744\nark://27927/pbd67rdsvh 0.008655877621028605\nhttp://www.jstor.org/stable/24493417 0.00861432566219732\nhttp://www.jstor.org/stable/20535405 0.008579466150662221\nhttp://www.jstor.org/stable/23470868 0.008506688515218689\nhttp://www.jstor.org/stable/25677791 0.008497611839062216\nark://27927/phx4f4kwp0s 0.00843448298572789\nark://27927/phz6jwb6f9k 0.008416310983710576\nark://27927/phzthkjzs6 0.00840427171789282\nark://27927/phx4f1dfxsb 0.008369620151127848\nark://27927/pbd6ddd1gz 0.008266223021329724\nark://27927/pbd188csh7m 0.008225987742143507\nark://27927/pbd1r35mtb2 0.00814441651923779\nhttp://www.jstor.org/stable/2693869 0.008092847034210843\nhttp://www.jstor.org/stable/2152896 0.008060432293352144\nhttp://www.jstor.org/stable/23032244 0.00802214522984386\nhttp://www.jstor.org/stable/23470866 0.008011662321880103\nark://27927/phx4f2m7r3x 0.00795281209023242\nark://27927/phx4h15t1wg 0.007935801476710207\nark://27927/phz16zhfvgg 0.00790101505230701\nhttp://www.jstor.org/stable/3597196 0.007845163466011463\nark://27927/phx4d7v270x 0.0077654226899051115\nark://27927/pgj2w93s7z 0.0076440125702945585\nark://27927/phx4gtgppnp 0.0075822281156032005\nark://27927/phx7qg6d536 0.007560550313138011\nhttp://www.jstor.org/stable/24512879 0.00754063989305467\nhttp://www.jstor.org/stable/20535999 0.007470502953916528\nark://27927/phzhpqkxxn5 0.007431711387527913\nark://27927/pbd1pj0rq1v 0.007365175904046198\nhttp://www.jstor.org/stable/4097941 0.007358571450752562\nhttp://www.jstor.org/stable/4146872 0.0073408242138754375\nhttp://www.jstor.org/stable/10.4169/000298910x480829 0.0073299481828671235\nark://27927/phx4djz5nft 0.007144915525853924\nark://27927/pbd6ckbb9h 0.007137535672666113\nark://27927/pbd6c8t2z1 0.007110523963884609\nark://27927/pbd6hg8bgg 0.007082992884696672\nark://27927/phx4h15tchk 0.007015487762931687\nark://27927/pbd98f39wx 0.006991295963725099\nark://27927/phx4d7958z5 0.006935652685360892\nark://27927/phzc72sz6pv 0.006861682742260464\nark://27927/pgj441vc51 0.006848793430123754\nhttp://www.jstor.org/stable/23032775 0.006824672860642611\nark://27927/pgj443726z 0.0068222983537917165\nark://27927/phx4dfxrqqs 0.006790612402711187\nark://27927/phzfs6cj9cp 0.006700187907833121\nark://27927/pbd67xrt6q 0.00667167677622019\nark://27927/phx4f1541gf 0.006670694065628787\nark://27927/pgk1m4xc1tx 0.006639172902474951\nark://27927/phz9c5ssgpn 0.006614530410344608\nhttp://www.jstor.org/stable/41639058 0.006614438114524079\nhttp://www.jstor.org/stable/4098294 0.006572196705793679\nark://27927/pbd69q23d0 0.006540770403281421\nhttp://www.jstor.org/stable/25433853 0.006489439761233311\nark://27927/phx29539tb3 0.006432885001344062\nark://27927/phx4h162qkp 0.006424140899562514\nark://27927/pbd69q2ddb 0.006406214868545526\nark://27927/pbd6fmthhg 0.006255320300695185\nark://27927/phz7ppqzbwb 0.006225708985218265\nark://27927/phzfkwzbxtw 0.006207028347592109\nhttp://www.jstor.org/stable/121063 0.006184986055408802\nhttp://www.jstor.org/stable/121062 0.006177476809484638\nark://27927/phx4dwn9hb4 0.006158152607723733\nark://27927/phw1691z2b 0.006122324021892946\nhttp://www.jstor.org/stable/24507483 0.006114679098457475\nhttp://www.jstor.org/stable/24507862 0.006091269198176152\nhttp://www.jstor.org/stable/10.4169/amer.math.monthly.122.03.217 0.006086591469526363\nark://27927/phx4f1dfz4c 0.006078450201007428\nark://27927/pbd6fmtcd0 0.006072336579359963\nhttp://www.jstor.org/stable/3845146 0.00604814796330904\nark://27927/phzjvkb46td 0.006016971090348386\nark://27927/phx4djz648s 0.005932252167821106\nark://27927/phx5wq0m2v3 0.005881139368287508\nark://27927/phx5qd2tk07 0.005874111863930538\nhttp://www.jstor.org/stable/23032787 0.005861378174203939\nark://27927/phx4h162skq 0.005839737710706496\nark://27927/phx4f26vx71 0.005755076087744283\nark://27927/phx4f5v0b91 0.005749661996110345\nark://27927/pbd1r35mngp 0.005741820538549545\nark://27927/phx4f5hk98p 0.005730092505998811\nark://27927/pgk1grw35d0 0.005677353500078459\nark://27927/phz2htzx5hc 0.005625163685766372\nark://27927/phz9t9j1m8x 0.005612004590981813\nark://27927/pgk11szkts1 0.005514467049638582\nhttp://www.jstor.org/stable/23757240 0.0054899792710785015\nark://27927/phx7dm58b0 0.005480083054009569\nhttp://www.jstor.org/stable/10.4169/mathhorizons.25.1.12 0.005414373517749293\nark://27927/phz2ftfb9g 0.005342327451913483\nhttp://www.jstor.org/stable/23813123 0.005305891882620527\nark://27927/phx4f5v0crq 0.0052977887856030985\nhttp://www.jstor.org/stable/2974922 0.005267758368545218\nark://27927/phx4gtgmvvm 0.005259223153176994\nhttp://www.jstor.org/stable/23513434 0.005244794311080801\nark://27927/phx4gtgk8mg 0.00523578981154309\nhttp://www.jstor.org/stable/23470865 0.005218623482796703\nark://27927/pbd98qx488 0.0051546506461616955\nark://27927/pgj445q8xt 0.005150378357414963\nhttp://www.jstor.org/stable/3072933 0.005146911993647471\nark://27927/phw252zwzh8 0.0051061990660146404\nark://27927/phz9c5jvmmf 0.005071913245303001\nhttp://www.jstor.org/stable/40590692 0.004922931485298896\nark://27927/pbd98f2t6c 0.0049161665894025385\nhttp://www.jstor.org/stable/10.4169/mathhorizons.25.2.5 0.004912362083860533\nhttp://www.jstor.org/stable/40345431 0.004908198978590208\nark://27927/pbd68jszm4 0.004835230494352646\nark://27927/phz9t9j1mrn 0.004689620056662912\nark://27927/pbd99d5kx5 0.0046810963876188156\nark://27927/phzb2cjwjtm 0.004651555531763792\nhttp://www.jstor.org/stable/23470871 0.004596984324974233\nark://27927/phx4f9x9wv1 0.004537991837978804\nark://27927/phx4dwnjb2h 0.004527147990683446\nark://27927/phz80pzj7g8 0.004480171887817083\nhttp://www.jstor.org/stable/24493721 0.004480028756456059\nark://27927/phzjx6fn98x 0.00446683650200645\nhttp://www.jstor.org/stable/23234166 0.004451997919222806\nark://27927/phx7wx5dnb4 0.0044105783195770455\nark://27927/pgjj7sjjpj 0.004397540559691112\nhttp://www.jstor.org/stable/25733386 0.004372942169250788\nark://27927/phx4dk57spt 0.004301906205990263\nhttp://www.jstor.org/stable/43302837 0.0042908942487644795\nark://27927/phz856vwhm6 0.00401012314934948\nark://27927/phx4f5v0c4s 0.0039938475288895355\nark://27927/pbd99d5nkz 0.0039689399893526565\nhttp://www.jstor.org/stable/26315451 0.003960208413916161\nark://27927/phx7jq28fkb 0.003938720728033828\nhttp://www.jstor.org/stable/3072914 0.0038811206480567485\nhttp://www.jstor.org/stable/43679172 0.0038593777522689717\nhttp://www.jstor.org/stable/20161875 0.0038008008424449675\nark://27927/pbd686xtnc 0.0037732620116109525\nhttp://www.jstor.org/stable/3650224 0.0037644068708393946\nark://27927/phzj6mbrfk2 0.003702576503208714\nhttp://www.jstor.org/stable/20160009 0.0036898111909124142\nhttp://www.jstor.org/stable/26409387 0.003390603903590044\nark://27927/pghjmff6xq 0.003386525829550353\nhttp://www.jstor.org/stable/2693797 0.003020260305829555\nark://27927/phx5q9wksq2 0.0029183824554901497\nhttp://www.jstor.org/stable/3597195 0.0024704727394879913\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"base_numbering":1,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":4}